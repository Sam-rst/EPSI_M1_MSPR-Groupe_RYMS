# Code Review - Pipeline ETL Load + Correction Encodage

**Date :** 2026-02-11
**Reviewer :** @rv (Code Reviewer)
**Feature :** ImplÃ©mentation complÃ¨te pipeline ETL + correction encodage UTF-8
**Commit range :** Modifications non committÃ©es

---

## ğŸ“‹ Vue d'Ensemble

**PÃ©rimÃ¨tre :** ImplÃ©mentation complÃ¨te de la pipeline ETL avec correction d'encodage
**Fichiers modifiÃ©s :** 4 fichiers
**Nouveaux modules :** Load (complet), Orchestrateur ETL, Migration Alembic
**Lignes ajoutÃ©es :** ~2000+ lignes
**ComplexitÃ© :** Moyenne-Ã‰levÃ©e

**Note Globale : 7.5/10**

---

## ğŸ“Š Analyse DÃ©taillÃ©e par Composant

### 1. **ModÃ¨le Base de DonnÃ©es** (`election_result.py`)

**Modifications :**
```python
# Colonnes rendues nullable
- nombre_inscrits: nullable=False â†’ nullable=True
- nombre_votants: nullable=False â†’ nullable=True
- nombre_exprimes: nullable=False â†’ nullable=True
- taux_participation: nullable=False â†’ nullable=True
```

**âœ… Points Positifs :**
- FlexibilitÃ© accrue pour donnÃ©es incomplÃ¨tes
- Migration Alembic propre et rÃ©versible

**âš ï¸ Points d'Attention :**
- **Impact sÃ©mantique** : Ces colonnes Ã©taient NOT NULL pour une raison. Autoriser NULL pourrait poser des problÃ¨mes dans les analyses ML futures
- **Validation manquante** : Aucune validation au niveau applicatif pour gÃ©rer les NULL
- **Documentation** : Le docstring du modÃ¨le n'a pas Ã©tÃ© mis Ã  jour

**Recommandation :**
```python
# Ajouter une validation au niveau Load
if pd.notna(row.get("nombre_inscrits")):
    # VÃ©rifier cohÃ©rence : inscrits >= votants >= exprimÃ©s
    validate_electoral_coherence(inscrits, votants, exprimes)
```

---

### 2. **Transform Elections** (`transform/core/elections.py`)

**Modifications Majeures :**
1. DÃ©tection automatique d'encodage (UTF-8 â†’ latin-1 fallback)
2. RÃ©Ã©criture complÃ¨te du parsing : agrÃ©gation â†’ dÃ©tail candidats
3. Pattern de colonnes rÃ©pÃ©titives (7 cols Ã— N candidats)

**âœ… Points Positifs :**
- **Robustesse encodage** : GÃ¨re les fichiers mixtes
- **Architecture correcte** : Parse le format SSMSI correctement
- **AgrÃ©gation** : Calculs de pourcentages corrects (voix/exprimÃ©s)

**âš ï¸ Points d'Attention :**

**A. DÃ©tection d'encodage fragile :**
```python
# PROBLÃˆME : Lit 1024 bytes puis rÃ©ouvre le fichier
try:
    with open(filepath, 'r', encoding='utf-8') as test_f:
        test_f.read(1024)  # âš ï¸ Peut ne pas dÃ©tecter erreur
except UnicodeDecodeError:
    encoding = 'latin-1'
```
- Ã‰chantillon de 1024 bytes peut manquer les caractÃ¨res problÃ©matiques
- Fichier ouvert 2 fois (performance)

**Meilleure approche :**
```python
import chardet

# DÃ©tection robuste
with open(filepath, 'rb') as f:
    raw = f.read(10000)  # Ã‰chantillon plus large
    detected = chardet.detect(raw)
    encoding = detected['encoding']
```

**B. Gestion erreurs insuffisante :**
```python
try:
    voix = parse_french_number(row[col + 4])
    # ...
except (IndexError, ValueError):
    break  # âš ï¸ Ignore silencieusement les erreurs
```
- Erreurs de parsing avalÃ©es sans log
- Difficile de dÃ©boguer en production

**C. Performance :**
```python
# Anti-pattern : AgrÃ©gation manuelle en boucle
for row in reader:
    if candidat not in candidats_data:
        candidats_data[candidat] = {'voix': 0}
    candidats_data[candidat]['voix'] += voix
```
- Pour Bordeaux (136 bureaux Ã— 11 candidats = 1496 itÃ©rations) : OK
- Si extension Ã  Gironde (33 communes Ã— N bureaux) : problÃ©matique

**Solution :**
```python
# Utiliser pandas groupby dÃ¨s le dÃ©part
df = pd.read_csv(filepath, sep=';', encoding=encoding)
df_bordeaux = df[df['Code du dÃ©partement'] == '33']
grouped = df_bordeaux.groupby(['annee', 'tour', 'candidat'])['voix'].sum()
```

---

### 3. **Transform SÃ©curitÃ©** (`transform/core/securite.py`)

**Modifications Majeures :**
1. Ajout mapping indicateurs granulaires â†’ catÃ©gories
2. AgrÃ©gation par (code_type, annÃ©e)
3. Calcul CRIMINALITE_TOTALE automatique

**âœ… Points Positifs :**
- **Mapping explicite** : Clair et maintenable
- **AgrÃ©gation correcte** : Utilise pandas groupby (performant)
- **Calcul dÃ©rivÃ©** : CRIMINALITE_TOTALE cohÃ©rent

**âš ï¸ Points d'Attention :**

**A. Mapping en dur dans le code :**
```python
MAPPING_INDICATEURS = {
    'Cambriolages de logement': 'VOLS_SANS_VIOLENCE',
    # ... âš ï¸ Hard-codÃ© dans la fonction
}
```
- Difficile Ã  maintenir si nouvelles catÃ©gories SSMSI
- Pas de traÃ§abilitÃ© des changements

**Meilleure approche :**
```python
# Externaliser dans config/mappings.py
SSMSI_CATEGORY_MAPPING = {
    "version": "2024-01",
    "source": "SSMSI",
    "mappings": {
        "VOLS_SANS_VIOLENCE": [
            "Cambriolages de logement",
            "Vols d'accessoires sur vÃ©hicules",
            # ...
        ]
    }
}
```

**B. Indicateurs non mappÃ©s ignorÃ©s silencieusement :**
```python
df_mapped = df_bordeaux[df_bordeaux['code_type'].notna()].copy()
# âš ï¸ Les stupÃ©fiants, escroqueries, etc. disparaissent sans trace
```

**Solution :**
```python
unmapped = df_bordeaux[df_bordeaux['code_type'].isna()]['indicateur'].unique()
if len(unmapped) > 0:
    logger.info(f"  Indicateurs non mappÃ©s (ignorÃ©s) : {list(unmapped)}")
```

---

### 4. **Module Load** (Nouveau)

**Structure :**
```
src/etl/load/
â”œâ”€â”€ main.py                    # Orchestrateur Load
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py            # Types indicateurs, config
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ elections.py           # Load rÃ©sultats Ã©lectoraux
â”‚   â”œâ”€â”€ indicateurs.py         # Load indicateurs sÃ©curitÃ©
â”‚   â”œâ”€â”€ territoire.py          # Load territoire
â”‚   â””â”€â”€ type_indicateur.py     # Load types
â””â”€â”€ utils/
    â””â”€â”€ validation.py          # Validations CSV
```

**âœ… Points Positifs :**
- **Architecture modulaire** : SÃ©paration des responsabilitÃ©s claire
- **Batch loading** : GÃ¨re les gros volumes (1000 rows/batch)
- **Gestion doublons** : VÃ©rifie l'existence avant insertion
- **Type casting** : Convertit id_territoire en string (Ã©vite erreurs)
- **Validation** : Valide les colonnes CSV avant chargement

**âš ï¸ Points d'Attention :**

**A. Absence de transactions explicites :**
```python
for i in range(0, len(df), BATCH_SIZE):
    batch_df = df.iloc[i : i + BATCH_SIZE]
    inserted = load_indicateurs_batch(session, batch_df, type_mapping)
    session.commit()  # âš ï¸ Commit par batch
```
- Si batch 3/5 Ã©choue, les batchs 1-2 sont dÃ©jÃ  committÃ©s
- Ã‰tat inconsistant difficile Ã  rollback

**Solution :**
```python
# Option 1: Transaction globale
with session.begin():
    for batch in batches:
        load_batch(session, batch)
    # Commit automatique si pas d'erreur

# Option 2: Savepoints
for batch in batches:
    savepoint = session.begin_nested()
    try:
        load_batch(session, batch)
        savepoint.commit()
    except Exception:
        savepoint.rollback()
        raise
```

**B. Gestion d'erreurs trop permissive :**
```python
if code_type not in type_mapping:
    print(f"[WARN]  Type inconnu ignorÃ© : {code_type}")
    continue  # âš ï¸ Continue silencieusement
```
- Typo dans code_type â†’ donnÃ©es perdues silencieusement
- Pas de compteur d'erreurs

**C. Performance du check de doublons :**
```python
# Pour chaque ligne, une requÃªte SQL
existing = session.query(Indicateur).filter(...).first()
```
- Pour 45 indicateurs Ã— 1 requÃªte = 45 queries
- Acceptable pour POC, mais pas scalable

**Solution :**
```python
# Charger tous les existants en mÃ©moire
existing_keys = set(
    session.query(
        Indicateur.id_territoire,
        Indicateur.id_type,
        Indicateur.annee,
        Indicateur.periode
    ).all()
)

# Check en mÃ©moire O(1)
if (id_territoire, id_type, annee, periode) in existing_keys:
    continue
```

---

### 5. **Orchestrateur ETL** (`etl/main.py`)

**âœ… Points Positifs :**
- **UX excellent** : Affichage clair, colorÃ©, progressif
- **Validation prÃ©alable** : VÃ©rifie PostgreSQL, tables, dossiers
- **Rapport dÃ©taillÃ©** : RÃ©sumÃ© final avec statistiques
- **Gestion erreurs** : Continue mÃªme si une phase Ã©choue partiellement

**âš ï¸ Points d'Attention :**

**A. Pas de gestion de retry :**
```python
success = extract_main()  # âš ï¸ Si Ã©chec rÃ©seau â†’ arrÃªt complet
```

**B. Logs console uniquement :**
- Pas de fichier de log persistant
- Difficile de dÃ©boguer les runs passÃ©s

**Solution :**
```python
import logging
from datetime import datetime

# Configurer logging vers fichier + console
log_file = f"logs/etl_{datetime.now():%Y%m%d_%H%M%S}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
```

---

### 6. **Migration Alembic**

**âœ… Points Positifs :**
- Migration manuelle (Ã©vite les bugs d'autogenerate PostGIS)
- Fonctions upgrade/downgrade symÃ©triques
- Nommage clair

**âš ï¸ Points d'Attention :**
- Pas de validation des donnÃ©es existantes avant migration
- Si des NULL existaient avant, la downgrade Ã©chouera

**Solution :**
```python
def downgrade() -> None:
    # VÃ©rifier qu'aucun NULL n'existe
    conn = op.get_bind()
    null_count = conn.execute(
        "SELECT COUNT(*) FROM election_result "
        "WHERE nombre_inscrits IS NULL"
    ).scalar()

    if null_count > 0:
        raise Exception(f"{null_count} rows with NULL - clean data first")

    # Puis alter columns...
```

---

## ğŸ”’ SÃ©curitÃ© & Risques

### âœ… Points SÃ©curisÃ©s :
1. **Pas d'injection SQL** : Utilisation d'ORM SQLAlchemy
2. **Validation colonnes** : VÃ©rifie prÃ©sence des colonnes attendues
3. **Type casting** : Conversion explicite des types

### âš ï¸ Risques IdentifiÃ©s :

**1. Path Traversal potentiel** (Faible risque)
```python
# Si filepath vient d'un input utilisateur non validÃ©
with open(filepath, 'r', encoding=encoding) as f:
```
- **Ici OK** : Chemins hardcodÃ©s dans config
- **Ã€ surveiller** : Si future feature "charger fichier custom"

**2. CSV Injection** (Faible risque)
```python
# Si un candidat s'appelle "=1+1" ou "=cmd|' /c calc'"
candidat = f"{prenom} {nom}"  # âš ï¸ Non sanitizÃ©
```
- **Ici OK** : DonnÃ©es sources officielles SSMSI
- **Ã€ surveiller** : Si affichage dans Excel/LibreOffice

**3. Pas de limite de taille de fichier**
```python
df = pd.read_csv(filepath, ...)  # âš ï¸ Peut charger 10GB en RAM
```
- **Ici OK** : Fichiers connus (~34MB max)
- **Ã€ surveiller** : Si extension Ã  d'autres sources

---

## ğŸ“ˆ Performance

### Points Critiques :

**1. Transform Elections : O(nÃ—m)**
- 136 bureaux Ã— 11 candidats Ã— 4 fichiers = ~6000 itÃ©rations
- **Acceptable** pour POC Bordeaux
- **ProblÃ©matique** si extension Gironde (Ã—30 communes)

**2. Load : N+1 queries**
- 1 SELECT par ligne pour check doublons
- **Acceptable** pour 72 lignes (27 Ã©lections + 45 indicateurs)
- **ProblÃ©matique** si >10k lignes

**3. DÃ©tection encodage : Double lecture**
- Fichier ouvert 2Ã— pour tester encodage
- Impact nÃ©gligeable pour fichiers 30MB

### Recommandations Performance :
```python
# Transform : Utiliser pandas dÃ¨s le dÃ©part
# Load : Bulk upsert PostgreSQL
from sqlalchemy.dialects.postgresql import insert

stmt = insert(Indicateur).values(records)
stmt = stmt.on_conflict_do_nothing(
    index_elements=['id_territoire', 'id_type', 'annee', 'periode']
)
session.execute(stmt)
```

---

## ğŸ“ Documentation & Tests

### âš ï¸ Manquements :

**1. Tests unitaires absents**
```
tests/
â””â”€â”€ etl/
    â”œâ”€â”€ test_transform_elections.py  # âŒ Manquant
    â”œâ”€â”€ test_load_indicateurs.py     # âŒ Manquant
    â””â”€â”€ test_encoding_detection.py   # âŒ Manquant
```

**Recommandation :**
```python
# tests/etl/test_transform_elections.py
def test_parse_candidats_2017():
    """VÃ©rifie parsing correct des 11 candidats 2017 T1."""
    result = transform_elections()
    assert result == True
    df = pd.read_csv('data/processed/elections/...')
    assert len(df[df['annee'] == 2017]) == 13  # 11+2

def test_encoding_accents():
    """VÃ©rifie que les accents sont prÃ©servÃ©s."""
    df = pd.read_csv('data/processed/elections/...')
    hamon = df[df['candidat'].str.contains('HAMON')]
    assert 'BenoÃ®t' in hamon['candidat'].values[0]
```

**2. Documentation API manquante**
- Pas de docstrings pour certaines fonctions Load
- Pas de schema des CSV attendus

**3. Logs insuffisants**
- Pas de logging structurÃ© (JSON)
- Pas de tracing des transformations (lineage)

---

## ğŸ¯ Recommandations Prioritaires

### ğŸ”´ Critique (Ã€ faire avant production)

1. **Ajouter transaction globale dans Load**
   - Ã‰viter Ã©tats inconsistants
   - Rollback automatique si erreur

2. **Logger les indicateurs non mappÃ©s**
   - TraÃ§abilitÃ© des donnÃ©es ignorÃ©es
   - Permet de complÃ©ter le mapping

3. **Valider cohÃ©rence Ã©lectorale**
   ```python
   assert nombre_votants <= nombre_inscrits
   assert nombre_exprimes <= nombre_votants
   assert sum(voix_candidats) == nombre_exprimes
   ```

### ğŸŸ¡ Important (Ã€ planifier)

4. **Externaliser mapping SSMSI**
   - Fichier JSON/YAML dans `config/`
   - VersionnÃ© avec date de MAJ

5. **Ajouter logging fichier**
   - Rotation automatique (max 10 fichiers)
   - Format structurÃ© (JSON) pour parsing

6. **Tests unitaires de base**
   - Au minimum : test_transform_elections
   - Au minimum : test_load_indicateurs

### ğŸŸ¢ AmÃ©liorations (Nice to have)

7. **Optimiser check doublons**
   - Charger existants en mÃ©moire
   - Ou utiliser UPSERT PostgreSQL

8. **Meilleure dÃ©tection encodage**
   - Utiliser chardet library
   - Cache du rÃ©sultat dÃ©tectÃ©

9. **Monitoring & ObservabilitÃ©**
   - MÃ©triques : durÃ©e par phase, lignes insÃ©rÃ©es/sec
   - Alertes si anomalies (0 lignes insÃ©rÃ©es, etc.)

---

## âœ… Verdict Final

**QualitÃ© GÃ©nÃ©rale : 7.5/10**

| CritÃ¨re | Note | Commentaire |
|---------|------|-------------|
| Architecture | 8/10 | Modulaire, sÃ©paration claire |
| Correctness | 8/10 | RÃ©sultats corrects, encodage fixÃ© |
| Robustesse | 6/10 | Gestion erreurs perfectible |
| Performance | 7/10 | OK pour POC, limites si scale |
| SÃ©curitÃ© | 8/10 | Pas de vulnÃ©rabilitÃ©s majeures |
| Documentation | 6/10 | Docstrings OK, tests absents |
| MaintenabilitÃ© | 7/10 | Code clair, mapping Ã  externaliser |

**Points Forts :**
- âœ… Architecture ETL solide et complÃ¨te
- âœ… ProblÃ¨me d'encodage rÃ©solu Ã©lÃ©gamment
- âœ… Code lisible et bien structurÃ©
- âœ… Gestion des doublons

**Points Faibles :**
- âŒ Absence de tests automatisÃ©s
- âŒ Gestion d'erreurs perfectible
- âŒ Transactions Load non atomiques
- âŒ Mapping hard-codÃ©

**Recommandation : MERGE AVEC RÃ‰SERVES**
- âœ… Code fonctionnel et prÃªt pour POC
- âš ï¸ ImplÃ©menter recommandations critiques avant production
- ğŸ“ CrÃ©er issues GitHub pour points d'amÃ©lioration

---

## ğŸ“Š MÃ©triques

- **Fichiers modifiÃ©s :** 4
- **Nouveaux fichiers :** 50+
- **Lignes ajoutÃ©es :** ~2000
- **Lignes supprimÃ©es :** ~200
- **ComplexitÃ© cyclomatique moyenne :** Moyenne
- **Couverture tests :** 0% âš ï¸

---

*Revue effectuÃ©e le : 2026-02-11*
*DurÃ©e de la revue : ~2h*
*Reviewer : @rv (Code Reviewer)*
