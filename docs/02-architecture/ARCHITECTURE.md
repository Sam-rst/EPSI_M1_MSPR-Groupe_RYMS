# Architecture ETL & Database - Electio-Analytics

**Version :** 3.0 (SchÃ©ma Enrichi Multi-GranularitÃ©s + Architecture Modulaire)

**Date :** 2026-02-11

**PÃ©rimÃ¨tre :** Bordeaux - PrÃ©sidentielles 2017 & 2022 (1er et 2nd tours) â†’ PrÃ©diction 2027

**Tech Lead :** @tech + @de + @ds

**DÃ©cisions Architecturales :**
- ADR-003 (Architecture ETL Modulaire Option 3)
- ADR-004 (Enrichissement SchÃ©ma Base de DonnÃ©es v3.0)

**Documents AssociÃ©s :**
- [MCD v3.0](./database/01-mcd.md) - ModÃ¨le Conceptuel de DonnÃ©es
- [MLD v3.0](./database/02-mld.md) - ModÃ¨le Logique de DonnÃ©es

---

## ðŸ—„ï¸ SchÃ©ma Base de DonnÃ©es v3.0 (NouveautÃ©)

### Vue d'Ensemble

Le schÃ©ma de base de donnÃ©es a Ã©tÃ© **enrichi et normalisÃ©** (3NF) pour supporter :
- **Multi-granularitÃ©s territoriales** (BUREAU, CANTON, COMMUNE, DEPARTEMENT, REGION)
- **RÃ©fÃ©rentiels candidats et partis** (tracking historique, profil idÃ©ologique)
- **SÃ©paration participation/rÃ©sultats** (Ã©limination redondances)
- **TraÃ§abilitÃ© granularitÃ©s** (table `election_territoire`)

**Gain ML estimÃ© :** +15-25% RÂ² score (Ã—3.5 features exploitables)

### Architecture

```mermaid
erDiagram
    REGION ||--o{ DEPARTEMENT : "contient"
    DEPARTEMENT ||--o{ CANTON : "contient"
    DEPARTEMENT ||--o{ COMMUNE : "contient"
    COMMUNE ||--o{ BUREAU_VOTE : "contient"

    CANDIDAT ||--o{ CANDIDAT_PARTI : "affilie"
    PARTI ||--o{ CANDIDAT_PARTI : "accueille"

    TYPE_ELECTION ||--o{ ELECTION : "categorise"
    ELECTION ||--o{ ELECTION_TERRITOIRE : "declare"
    ELECTION_TERRITOIRE ||--o{ RESULTAT_PARTICIPATION : "valide"
    ELECTION_TERRITOIRE ||--o{ RESULTAT_CANDIDAT : "valide"
    CANDIDAT ||--o{ RESULTAT_CANDIDAT : "obtient"
```

### Tables Principales (14)

| Domaine | Tables | RÃ´le |
|---------|--------|------|
| **GÃ©ographique** | region, departement, canton, commune, arrondissement, bureau_vote | HiÃ©rarchie multi-niveaux |
| **Candidats/Partis** | candidat, parti, candidat_parti | RÃ©fÃ©rentiels normalisÃ©s |
| **Ã‰lections** | type_election, election, election_territoire | Ã‰vÃ©nements + rÃ©fÃ©rentiel granularitÃ©s |
| **RÃ©sultats** | resultat_participation, resultat_candidat | SÃ©paration stats gÃ©nÃ©rales vs candidats |

### Features ML Exploitables

**Avant (schÃ©ma v2.0) :** ~10 features
```
['nombre_voix', 'pourcentage_voix', 'criminalite_totale', ...]
```

**AprÃ¨s (schÃ©ma v3.0) :** ~35 features
```python
# Candidat (7)
['age', 'nb_elections_precedentes', 'score_moyen_historique',
 'evolution_momentum', 'profession', ...]

# Parti (6)
['position_economique', 'position_sociale', 'classification_ideologique',
 'distance_ideologique_gagnant', ...]

# Participation (8)
['taux_abstention', 'taux_blancs_nuls', 'evolution_vs_N-1',
 'ecart_vs_national', ...]

# GÃ©ographique (5)
['type_territoire', 'densite_population', 'taille_commune', ...]

# Socio-Ã©conomique (9)
['criminalite_totale', 'criminalite_evolution', ...]
```

**RÃ©fÃ©rence complÃ¨te :** [MCD v3.0](./database/01-mcd.md)

---

## Vue d'Ensemble ETL

Le pipeline ETL (Extract-Transform-Load) centralise les donnÃ©es Ã©lectorales (1er et 2nd tours des prÃ©sidentielles 2017 & 2022) et socio-Ã©conomiques depuis 3 sources externes, en garantissant la **traÃ§abilitÃ©**, la **qualitÃ©** et la **reproductibilitÃ©**.

**Nouvelle architecture :** Le module ETL a Ã©tÃ© refactorisÃ© selon l'**Architecture Option 3** (sÃ©paration par type de fonction) pour une scalabilitÃ© et maintenabilitÃ© maximales.

---

## ðŸ—ï¸ Architecture Modulaire (Version 2.0)

### Structure du Module ETL

```
src/etl/
â”œâ”€â”€ extract/                    # Extraction des donnÃ©es brutes
â”‚   â”œâ”€â”€ config/                # Configuration centralisÃ©e
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py        # URLs, chemins, constantes
â”‚   â”œâ”€â”€ core/                  # Logique mÃ©tier par source
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ elections.py       # TÃ©lÃ©chargement Ã©lections
â”‚   â”‚   â””â”€â”€ securite.py        # TÃ©lÃ©chargement sÃ©curitÃ©
â”‚   â”œâ”€â”€ utils/                 # Utilitaires gÃ©nÃ©riques
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ download.py        # download_file()
â”‚   â”œâ”€â”€ __init__.py            # Exports publics
â”‚   â””â”€â”€ main.py                # Orchestrateur extraction
â”‚
â”œâ”€â”€ transform/                  # Transformation des donnÃ©es
â”‚   â”œâ”€â”€ config/                # Configuration centralisÃ©e
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py        # Chemins, constantes
â”‚   â”œâ”€â”€ core/                  # Logique mÃ©tier par source
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ elections.py       # Transformation Ã©lections
â”‚   â”‚   â””â”€â”€ securite.py        # Transformation sÃ©curitÃ©
â”‚   â”œâ”€â”€ utils/                 # Utilitaires de parsing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ parsing.py         # parse_french_number()
â”‚   â”œâ”€â”€ __init__.py            # Exports publics
â”‚   â””â”€â”€ main.py                # Orchestrateur transformation
â”‚
â””â”€â”€ README.md                   # Documentation complÃ¨te
```

### Principes Architecturaux

1. **SÃ©paration des responsabilitÃ©s (SRP)**
   - `config/` : Configuration uniquement (URLs, chemins, constantes)
   - `core/` : Logique mÃ©tier spÃ©cifique Ã  chaque source de donnÃ©es
   - `utils/` : Fonctions gÃ©nÃ©riques rÃ©utilisables
   - `main.py` : Orchestration pure sans logique mÃ©tier

2. **ScalabilitÃ©**
   - Ajout d'une nouvelle source = 1 fichier dans `core/`
   - Pas de modification des modules existants (Open/Closed Principle)

3. **TestabilitÃ©**
   - Chaque module peut Ãªtre testÃ© indÃ©pendamment
   - Imports isolÃ©s facilitent les mocks et stubs

4. **RÃ©utilisabilitÃ©**
   - Utilitaires dans `utils/` rÃ©utilisables partout
   - API publique claire via `__init__.py`

5. **MaintenabilitÃ©**
   - Code organisÃ© et documentÃ©
   - Type hints sur toutes les fonctions
   - Docstrings au format Google

### Avantages de l'Architecture Option 3

| Aspect | Avant | AprÃ¨s |
|--------|-------|-------|
| **Fichiers racine** | 6 fichiers | 1 fichier (main.py) + 3 packages |
| **ScalabilitÃ©** | Moyenne | Excellente |
| **TestabilitÃ©** | Bonne | Parfaite |
| **Patterns** | Simple | Enterprise-grade |
| **Lignes de code** | 621 lignes | 1220 lignes (mieux organisÃ©es) |

**RÃ©fÃ©rence :** Voir `docs/02-architecture/adr/ADR-003-architecture-modulaire-etl.md`

```mermaid
flowchart LR
    A[Sources Externes] --> B[Extraction]
    B --> C[Transformation]
    C --> D[Chargement]

    style A fill:#e1f5ff,stroke:#01579b, color: #020202
    style B fill:#fff3e0,stroke:#e65100, color: #020202
    style C fill:#f3e5f5,stroke:#4a148c, color: #020202
    style D fill:#e8f5e9,stroke:#1b5e20, color: #020202
```

---

## Architecture Globale

```mermaid
graph LR
    subgraph Sources Externes
        A[data.gouv.fr<br/>Ã‰lections 2017/2022]
        B[SSMSI<br/>SÃ©curitÃ© 2017-2024]
        C[INSEE<br/>Emploi IRIS 2017-2024]
    end

    subgraph ETL Pipeline
        D[Extract<br/>extract_*.py]
        E[Transform<br/>transform.py]
        F[Load<br/>load.py]
    end

    subgraph Stockage
        G[(PostgreSQL<br/>electio_analytics)]
        H[/data/raw/<br/>CSV bruts/]
        I[/data/processed/<br/>CSV nettoyÃ©s/]
    end

    A --> D
    B --> D
    C --> D
    D --> H
    H --> E
    E --> I
    I --> F
    F --> G
```

---

## Phase 1 : EXTRACT (Extraction)

### Objectif
TÃ©lÃ©charger les datasets bruts depuis les APIs/sites open data et les sauvegarder localement.

### Sources de DonnÃ©es

#### Source 1 : Ã‰lections PrÃ©sidentielles (data.gouv.fr)

**DonnÃ©es collectÃ©es :** 1er et 2nd tours pour 2017 et 2022 (4 fichiers CSV)

**URL :**
- 2017 Tour 1 : `https://www.data.gouv.fr/datasets/elections-presidentielle-2017-resultats-bureaux-vote-tour-1`
- 2017 Tour 2 : `https://www.data.gouv.fr/datasets/5cddfde49ce2e76d93bdb18b`
- 2022 Tour 1 : `https://www.data.gouv.fr/datasets/election-presidentielle-des-10-et-24-avril-2022-resultats-du-1er-tour`
- 2022 Tour 2 : `https://www.data.gouv.fr/datasets/election-presidentielle-des-10-et-24-avril-2022-resultats-du-second-tour`

**Format :** CSV
**GranularitÃ© :** Bureau de vote
**Champs clÃ©s :**
- `Code du dÃ©partement`, `Code de la commune`, `Code du bureau de vote`
- `Nom`, `PrÃ©nom` (candidat)
- `Voix`, `% Voix/Exp`, `Inscrits`, `Votants`, `ExprimÃ©s`

**Filtrage :** DÃ©partement 33 (Gironde), Commune 33063 (Bordeaux)

**Script :** `src/etl/extract_elections.py`

```python
def extract_elections(year: int, tour: int, output_path: str) -> pd.DataFrame:
    """
    TÃ©lÃ©charge les rÃ©sultats prÃ©sidentielles depuis data.gouv.fr.

    Args:
        year: AnnÃ©e Ã©lection (2017, 2022)
        tour: Tour de l'Ã©lection (1 ou 2)
        output_path: Chemin sauvegarde CSV brut

    Returns:
        DataFrame avec rÃ©sultats bruts

    Notes:
        TÃ©lÃ©charge les 4 fichiers requis :
        - 2017 Tour 1, 2017 Tour 2
        - 2022 Tour 1, 2022 Tour 2
    """
    # TÃ©lÃ©charger CSV depuis data.gouv.fr
    # Filtrer dÃ©partement=33, commune=33063
    # Sauvegarder dans /data/raw/elections_{year}_tour{tour}.csv
    pass
```

---

#### Source 2 : SÃ©curitÃ© / CriminalitÃ© (SSMSI)

**URL :** `https://www.data.gouv.fr/fr/datasets/bases-statistiques-communale-departementale-et-regionale-de-la-delinquance-enregistree-par-la-police-et-la-gendarmerie-nationales/`

**Format :** CSV
**GranularitÃ© :** Commune (pas de dÃ©tail IRIS public)
**PÃ©riode :** 2017-2024
**Champs clÃ©s :**
- `Code.dÃ©partement`, `Code.commune`, `AnnÃ©e`, `Mois` (optionnel)
- 13 indicateurs : `Cambriolages de logement`, `Vols de vÃ©hicules`, `Coups et blessures volontaires`, etc.

**Limitation :** DonnÃ©es communales uniquement (agrÃ©gation nÃ©cessaire pour IRIS)

**Script :** `src/etl/extract_securite.py`

```python
def extract_securite(start_year: int, end_year: int, output_path: str) -> pd.DataFrame:
    """
    TÃ©lÃ©charge les statistiques SSMSI pour Bordeaux.

    Args:
        start_year: AnnÃ©e dÃ©but (2017)
        end_year: AnnÃ©e fin (2024)
        output_path: Chemin sauvegarde CSV brut

    Returns:
        DataFrame avec faits de dÃ©linquance
    """
    # TÃ©lÃ©charger CSV SSMSI
    # Filtrer commune=33063
    # Sauvegarder dans /data/raw/securite_2017_2024.csv
    pass
```

---

#### Source 3 : Emploi / ChÃ´mage (INSEE)

**URL :**
- DonnÃ©es IRIS : `https://www.insee.fr/fr/statistiques/zones/2011101` (Dossier complet commune)
- Demandeurs emploi IRIS : `https://www.insee.fr/fr/statistiques/7654804`

**Format :** CSV, Excel
**GranularitÃ© :** IRIS (Ilots RegroupÃ©s pour l'Information Statistique)
**Champs clÃ©s :**
- `CODE_IRIS`, `NOM_IRIS`, `AnnÃ©e`, `Trimestre`
- `Taux de chÃ´mage`, `Population active`, `Revenus mÃ©dian`, `Nombre emplois`

**Script :** `src/etl/extract_emploi.py`

```python
def extract_emploi_iris(commune_code: str, output_path: str) -> pd.DataFrame:
    """
    TÃ©lÃ©charge les donnÃ©es emploi INSEE au niveau IRIS.

    Args:
        commune_code: Code commune INSEE (33063)
        output_path: Chemin sauvegarde CSV brut

    Returns:
        DataFrame avec indicateurs emploi par IRIS
    """
    # Scraper ou utiliser API INSEE
    # Extraire donnÃ©es IRIS pour Bordeaux
    # Sauvegarder dans /data/raw/emploi_iris_bordeaux.csv
    pass
```

---

### Arborescence de Sortie (Extract)

```
/data/raw/elections/
    â”œâ”€â”€ presidentielles_2017_tour1_bureaux_vote.csv  (1er tour 2017)
    â”œâ”€â”€ presidentielles_2017_tour2_bureaux_vote.csv  (2nd tour 2017)
    â”œâ”€â”€ presidentielles_2022_tour1_bureaux_vote.csv  (1er tour 2022)
    â”œâ”€â”€ presidentielles_2022_tour2_bureaux_vote.csv  (2nd tour 2022)

/data/raw/securite/
    â””â”€â”€ delinquance_bordeaux_2017_2024.csv

/data/raw/emploi/
    â””â”€â”€ emploi_iris_bordeaux_2017_2024.csv
```

---

## Phase 2 : TRANSFORM (Transformation)

### Objectif
Nettoyer, harmoniser et enrichir les donnÃ©es brutes pour les rendre exploitables.

### Transformations ClÃ©s

#### T1 : Harmonisation GÃ©ographique

**ProblÃ¨me :** 3 granularitÃ©s diffÃ©rentes
- Ã‰lections : **Bureau de vote** (Code BV)
- SÃ©curitÃ© : **Commune** (33063)
- Emploi : **IRIS** (Code IRIS)

**Solution :** CrÃ©er une table de mapping `bureau_vote â†’ IRIS`

**Approche :**
1. Utiliser le fichier de correspondance INSEE : [Table passage Bureau de vote â†’ IRIS](https://www.insee.fr/fr/information/2008354)
2. GÃ©ocoder les adresses de bureaux de vote (lat/long)
3. Spatial join avec polygones IRIS via PostGIS

**Script :** `src/etl/transform_geo_mapping.py`

```python
def map_bureau_to_iris(elections_df: pd.DataFrame,
                        mapping_table: pd.DataFrame) -> pd.DataFrame:
    """
    Associe chaque bureau de vote Ã  un IRIS.

    Args:
        elections_df: RÃ©sultats Ã©lectoraux par bureau
        mapping_table: Table Bureau â†’ IRIS (INSEE)

    Returns:
        DataFrame avec colonne `id_iris` ajoutÃ©e
    """
    # Jointure elections_df <> mapping_table sur code_bureau
    # Ajouter colonne `id_iris`
    pass
```

---

#### T2 : Normalisation des DonnÃ©es Ã‰lectorales

**Transformations :**
- Uniformiser noms candidats : `"MACRON Emmanuel"` â†’ `"Emmanuel MACRON"`
- Calculer `taux_participation = (Votants / Inscrits) * 100`
- Calculer `pourcentage_voix = (Voix / ExprimÃ©s) * 100`
- GÃ©rer valeurs manquantes (bureaux fermÃ©s, donnÃ©es incomplÃ¨tes)
- DÃ©doublonner (si CSV mal formatÃ©s)

**Script :** `src/etl/transform_elections.py`

```python
def normalize_elections(raw_df: pd.DataFrame) -> pd.DataFrame:
    """
    Nettoie et normalise les rÃ©sultats Ã©lectoraux.

    Returns:
        DataFrame conforme au schÃ©ma Election_Result (MCD)
    """
    # Renommer colonnes selon MCD
    # Calculer mÃ©triques dÃ©rivÃ©es (%, taux)
    # Typer les colonnes (int, float, str)
    pass
```

---

#### T3 : Normalisation SÃ©curitÃ©

**Transformations :**
- Pivot des 13 indicateurs (colonnes â†’ lignes) : `type_fait`, `nombre_faits`
- Calculer `taux_pour_1000_hab = (nombre_faits / population) * 1000`
- GÃ©rer valeurs nulles (certains types de faits absents)
- Harmoniser types de faits (uniformiser libellÃ©s)

**Script :** `src/etl/transform_securite.py`

```python
def normalize_securite(raw_df: pd.DataFrame,
                        population_df: pd.DataFrame) -> pd.DataFrame:
    """
    Transforme les donnÃ©es SSMSI au format long (pivot).

    Returns:
        DataFrame conforme au schÃ©ma Indicateur_Securite (MCD)
    """
    # Pivot : 13 colonnes indicateurs â†’ 13 lignes par annÃ©e
    # Calculer taux pour 1000 habitants
    pass
```

---

#### T4 : Normalisation Emploi

**Transformations :**
- GÃ©rer donnÃ©es trimestrielles vs annuelles (agrÃ©ger ou interpoler)
- Calculer `taux_chomage = (Chomeurs / Population_active) * 100`
- GÃ©rer valeurs aberrantes (taux > 100%, nÃ©gatifs)
- Harmoniser codes IRIS (supprimer prÃ©fixes/suffixes)

**Script :** `src/etl/transform_emploi.py`

```python
def normalize_emploi(raw_df: pd.DataFrame) -> pd.DataFrame:
    """
    Nettoie les indicateurs emploi INSEE.

    Returns:
        DataFrame conforme au schÃ©ma Indicateur_Emploi (MCD)
    """
    # Harmoniser codes IRIS
    # Calculer taux de chÃ´mage
    # GÃ©rer valeurs manquantes (imputation mÃ©diane ou suppression)
    pass
```

---

#### T5 : Enrichissement Temporel

**Objectif :** CrÃ©er features pour le ML

**Features dÃ©rivÃ©es :**
- `evolution_chomage_1an = taux_chomage(N) - taux_chomage(N-1)`
- `evolution_criminalite_3ans = AVG(faits_2020-2022) - AVG(faits_2017-2019)`
- `tendance_participation = participation(2022) - participation(2017)`

**Script :** `src/etl/transform_features.py`

```python
def create_temporal_features(emploi_df: pd.DataFrame,
                               securite_df: pd.DataFrame) -> pd.DataFrame:
    """
    CrÃ©e des features d'Ã©volution temporelle.

    Returns:
        DataFrame avec colonnes delta_* et tendance_*
    """
    # Calculer variations annÃ©e N vs N-1
    # Moyennes mobiles sur 3 ans
    pass
```

---

### Arborescence de Sortie (Transform)

```
/data/processed/
    â”œâ”€â”€ territoire.csv                 (rÃ©fÃ©rentiel IRIS + Bureaux)
    â”œâ”€â”€ elections_normalized.csv       (rÃ©sultats harmonisÃ©s)
    â”œâ”€â”€ securite_normalized.csv        (indicateurs pivotÃ©s)
    â”œâ”€â”€ emploi_normalized.csv          (taux chÃ´mage nettoyÃ©s)
    â””â”€â”€ features_ml.csv                (features enrichies pour ML)
```

---

## Phase 3 : LOAD (Chargement)

### Objectif
InsÃ©rer les donnÃ©es transformÃ©es dans PostgreSQL en garantissant l'intÃ©gritÃ© rÃ©fÃ©rentielle.

### Ordre de Chargement (Respect des FK)

```mermaid
flowchart TD
    A[1. Territoire<br/>table parent, pas de FK]
    B[2. Election_Result<br/>FK â†’ Territoire]
    C[3. Indicateur_Securite<br/>FK â†’ Territoire]
    D[4. Indicateur_Emploi<br/>FK â†’ Territoire]
    E[5. Prediction<br/>FK â†’ Territoire, chargÃ© en Phase 4]

    A --> B
    B --> C
    C --> D
    D --> E

    style A fill:#e1f5ff,stroke:#01579b, color: #020202
    style B fill:#fff3e0,stroke:#e65100, color: #020202
    style C fill:#f3e5f5,stroke:#4a148c, color: #020202
    style D fill:#fff3e0,stroke:#e65100, color: #020202
    style E fill:#e8f5e9,stroke:#1b5e20, color: #020202
```

### Scripts de Chargement

**Script principal :** `src/etl/load.py`

```python
import pandas as pd
import sqlalchemy as sa

def load_to_postgres(df: pd.DataFrame,
                      table_name: str,
                      engine: sa.Engine,
                      if_exists: str = 'append') -> int:
    """
    Charge un DataFrame dans PostgreSQL.

    Args:
        df: DataFrame Ã  charger
        table_name: Nom de la table cible
        engine: SQLAlchemy engine
        if_exists: 'append', 'replace', 'fail'

    Returns:
        Nombre de lignes insÃ©rÃ©es
    """
    df.to_sql(table_name, engine, if_exists=if_exists, index=False)
    return len(df)

def run_etl_pipeline():
    """ExÃ©cute le pipeline ETL complet."""

    # 1. Connexion PostgreSQL
    engine = sa.create_engine('postgresql://admin:password@localhost:5432/electio_analytics')

    # 2. Chargement Territoire
    territoire_df = pd.read_csv('/data/processed/territoire.csv')
    load_to_postgres(territoire_df, 'territoire', engine, if_exists='replace')

    # 3. Chargement Election_Result
    elections_df = pd.read_csv('/data/processed/elections_normalized.csv')
    load_to_postgres(elections_df, 'election_result', engine)

    # 4. Chargement Indicateur_Securite
    securite_df = pd.read_csv('/data/processed/securite_normalized.csv')
    load_to_postgres(securite_df, 'indicateur_securite', engine)

    # 5. Chargement Indicateur_Emploi
    emploi_df = pd.read_csv('/data/processed/emploi_normalized.csv')
    load_to_postgres(emploi_df, 'indicateur_emploi', engine)

    print("âœ… ETL Pipeline terminÃ© avec succÃ¨s")
```

---

### Validation Post-Chargement

**Script :** `src/etl/validate_data.py`

```python
def validate_referential_integrity(engine: sa.Engine) -> bool:
    """
    VÃ©rifie que toutes les FK sont valides.

    Returns:
        True si intÃ©gritÃ© OK, False sinon
    """
    # Test 1 : Tous les id_territoire de Election_Result existent dans Territoire
    query = """
        SELECT COUNT(*)
        FROM election_result er
        LEFT JOIN territoire t ON er.id_territoire = t.id_territoire
        WHERE t.id_territoire IS NULL
    """
    result = pd.read_sql(query, engine)
    orphans = result.iloc[0, 0]

    if orphans > 0:
        print(f"âŒ {orphans} rÃ©sultats Ã©lectoraux sans territoire associÃ©")
        return False

    # Test 2 : Pas de doublons
    # Test 3 : Pas de valeurs nulles sur colonnes NOT NULL
    # ...

    print("âœ… IntÃ©gritÃ© rÃ©fÃ©rentielle validÃ©e")
    return True
```

---

## Gestion des Erreurs & TraÃ§abilitÃ©

### Logs ETL

**Structure :** Chaque Ã©tape du pipeline gÃ©nÃ¨re un log JSON

```json
{
  "timestamp": "2026-02-09T14:32:10Z",
  "step": "extract_elections",
  "status": "success",
  "rows_extracted": 12450,
  "source_url": "https://data.gouv.fr/...",
  "output_file": "/data/raw/elections_2022_tour2.csv",
  "execution_time_seconds": 3.2
}
```

**Script :** `src/etl/logger.py`

```python
import json
from datetime import datetime

def log_etl_step(step: str, status: str, metadata: dict):
    """Enregistre les mÃ©tadonnÃ©es d'une Ã©tape ETL."""
    log_entry = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "step": step,
        "status": status,
        **metadata
    }

    with open('/logs/etl_pipeline.jsonl', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')
```

---

### Gestion des Erreurs

**StratÃ©gie :** Rollback transactionnel en cas d'Ã©chec

```python
def run_etl_with_rollback():
    engine = sa.create_engine('postgresql://...')

    with engine.begin() as conn:  # Transaction automatique
        try:
            # Ã‰tape 1 : Extract
            extract_elections(2017, '/data/raw/elections_2017.csv')

            # Ã‰tape 2 : Transform
            transform_elections('/data/raw/elections_2017.csv')

            # Ã‰tape 3 : Load
            load_to_postgres(elections_df, 'election_result', conn)

            # Si tout rÃ©ussit â†’ COMMIT automatique

        except Exception as e:
            # Si erreur â†’ ROLLBACK automatique
            log_etl_step('run_etl', 'failed', {'error': str(e)})
            raise
```

---

## Diagramme de Flux Complet

```mermaid
sequenceDiagram
    participant User
    participant Extract as Extract Scripts
    participant Raw as /data/raw/
    participant Transform as Transform Scripts
    participant Processed as /data/processed/
    participant Load as Load Script
    participant DB as PostgreSQL

    User->>Extract: python src/etl/extract_all.py
    Extract->>Raw: TÃ©lÃ©charge CSVs (Ã‰lections, SÃ©curitÃ©, Emploi)
    Raw-->>Transform: Lit CSVs bruts
    Transform->>Transform: Nettoyage + Harmonisation
    Transform->>Processed: Sauvegarde CSVs normalisÃ©s
    Processed-->>Load: Lit CSVs processÃ©s
    Load->>DB: INSERT INTO territoire, election_result, ...
    DB-->>Load: Validation intÃ©gritÃ© FK
    Load-->>User: âœ… ETL terminÃ© (26k lignes chargÃ©es)
```

---

## Architecture Modulaire

### Structure des Modules

```
src/etl/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ extract/
    â”‚   â”œâ”€â”€ extract_elections.py      (TÃ©lÃ©charge rÃ©sultats Ã©lectoraux)
    â”‚   â”œâ”€â”€ extract_securite.py       (TÃ©lÃ©charge donnÃ©es SSMSI)
    â”‚   â”œâ”€â”€ extract_emploi.py         (TÃ©lÃ©charge donnÃ©es INSEE)
    â”‚   â””â”€â”€ extract_all.py            (Orchestrateur : lance tous les extracts)
    â”‚
    â”œâ”€â”€ transform/
    â”‚   â”œâ”€â”€ transform_elections.py    (Normalise Ã©lections)
    â”‚   â”œâ”€â”€ transform_securite.py     (Normalise sÃ©curitÃ©)
    â”‚   â”œâ”€â”€ transform_emploi.py       (Normalise emploi)
    â”‚   â”œâ”€â”€ transform_geo_mapping.py  (Mapping Bureau â†’ IRIS)
    â”‚   â”œâ”€â”€ transform_features.py     (Features ML)
    â”‚   â””â”€â”€ transform_all.py          (Orchestrateur transforms)
    â”‚
    â”œâ”€â”€ load/
    â”‚   â”œâ”€â”€ load.py                   (Chargement PostgreSQL)
    â”‚   â””â”€â”€ validate_data.py          (Tests intÃ©gritÃ© post-load)
    â”‚
    â”œâ”€â”€ utils/
    â”‚   â”œâ”€â”€ logger.py                 (Logs ETL)
    â”‚   â”œâ”€â”€ db_connection.py          (SQLAlchemy engine)
    â”‚   â””â”€â”€ config.py                 (Variables d'environnement)
    â”‚
    â””â”€â”€ main.py                       (Point d'entrÃ©e : lance pipeline complet)
```

---

## Configuration (Variables d'Environnement)

**Fichier :** `.env` (non versionnÃ©)

```bash
# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=electio_analytics
POSTGRES_USER=admin
POSTGRES_PASSWORD=secure_password

# Chemins
DATA_RAW_PATH=/data/raw
DATA_PROCESSED_PATH=/data/processed
LOGS_PATH=/logs

# URLs Data.gouv.fr
ELECTIONS_2017_URL=https://www.data.gouv.fr/fr/datasets/r/...
ELECTIONS_2022_URL=https://www.data.gouv.fr/fr/datasets/r/...
SECURITE_URL=https://www.data.gouv.fr/fr/datasets/r/...
EMPLOI_INSEE_URL=https://www.insee.fr/...

# Filtres
DEPARTEMENT_CODE=33
COMMUNE_CODE=33063
COMMUNE_NAME=Bordeaux
```

---

## ExÃ©cution du Pipeline

### Commande Unique (Orchestration)

```bash
# Lancer le pipeline complet (Extract â†’ Transform â†’ Load)
python src/etl/main.py --full

# Lancer uniquement Extract
python src/etl/main.py --extract

# Lancer uniquement Transform
python src/etl/main.py --transform

# Lancer uniquement Load
python src/etl/main.py --load
```

### Docker Compose (Environnement complet)

**Fichier :** `docker-compose.yml`

```yaml
version: '3.8'

services:
  postgres:
    image: postgis/postgis:15-3.3
    environment:
      POSTGRES_DB: electio_analytics
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: secure_password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docs/MCD.sql:/docker-entrypoint-initdb.d/01_schema.sql

  etl:
    build: .
    depends_on:
      - postgres
    environment:
      POSTGRES_HOST: postgres
    volumes:
      - ./data:/data
      - ./logs:/logs
    command: python src/etl/main.py --full

volumes:
  postgres_data:
```

**Lancement :**
```bash
docker-compose up -d postgres    # DÃ©marre PostgreSQL
docker-compose up etl            # Lance le pipeline ETL
```

---

## MÃ©triques & Monitoring

### MÃ©triques de Performance

| Ã‰tape | Temps cible | VolumÃ©trie |
|-------|-------------|------------|
| **Extract Elections** | <10s | 12k lignes (4 fichiers CSV) |
| **Extract SÃ©curitÃ©** | <5s | 1k lignes |
| **Extract Emploi** | <15s | 400 lignes (50 IRIS Ã— 8 ans) |
| **Transform All** | <20s | Normalisation + Mapping gÃ©o |
| **Load All** | <30s | 26k insertions PostgreSQL |
| **Total Pipeline** | **<80s** | |

### Dashboard Monitoring (Optionnel - Phase future)

- Grafana + Prometheus pour tracer les exÃ©cutions ETL
- Alertes si Ã©checs (Slack/Email)

---

## Ã‰volutions Futures (Hors POC)

1. **Automatisation :** Scheduler (Airflow, Prefect) pour refresh automatique donnÃ©es
2. **IncrÃ©mental ETL :** Ne recharger que les nouvelles donnÃ©es (delta)
3. **Data Quality Tests :** Great Expectations pour validation automatique
4. **Streaming :** Kafka pour ingestion temps rÃ©el (sondages)
5. **Data Lineage :** Apache Atlas pour tracer l'origine de chaque donnÃ©e

---

## Checklist de Validation

- [ ] Toutes les tables PostgreSQL crÃ©Ã©es (5 tables)
- [ ] IntÃ©gritÃ© rÃ©fÃ©rentielle validÃ©e (aucune FK orpheline)
- [ ] VolumÃ©trie conforme (~26k lignes)
- [ ] Logs ETL gÃ©nÃ©rÃ©s (1 fichier JSONL)
- [ ] Pas de valeurs nulles sur colonnes NOT NULL
- [ ] Pas de doublons (contraintes UNIQUE respectÃ©es)
- [ ] Codes gÃ©ographiques cohÃ©rents (INSEE, IRIS, Bureaux)
- [ ] Pipeline reproductible (exÃ©cution depuis zÃ©ro rÃ©ussie)

---

## RÃ©fÃ©rences Techniques

- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [SQLAlchemy ORM](https://docs.sqlalchemy.org/)
- [Data.gouv.fr API](https://doc.data.gouv.fr/)
- [INSEE Web Services](https://api.insee.fr/)
- [PostGIS Spatial Queries](https://postgis.net/docs/)

---

**Statut :** âœ… Documentation complÃ©tÃ©e
**Prochaine Ã©tape :** Phase 3 - `@de` implÃ©mente les scripts ETL (`extract_*.py`, `transform_*.py`, `load.py`)
